{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188fc056",
   "metadata": {},
   "source": [
    "# Machine Learning-Powered Video Library\n",
    "**By [Czarina Luna](https://www.linkedin.com/in/czarinagluna/)**\n",
    "***\n",
    "\n",
    "Video sharing applications today lack the functionality for users to search videos by their content. As a solution I developed a searchable video library that processes videos and returns exact matches to queries using machine learning and artificial intelligence including speech recognition, optical character recognition, and object detection. \n",
    "\n",
    "*[Link to Web Application](https://share.streamlit.io/czarinagluna/ml-powered-video-library/main)*\n",
    "\n",
    "### Contents\n",
    "* [Business Problem](#Business-Problem)\n",
    "* [Data and Methodology](#Data-and-Methodology)\n",
    "* [Video Processing](#Video-Processing)\n",
    "    * [I. Audio Processing](#I.-Audio-Processing)\n",
    "    * [II. Extracting Visual Text](#II.-Extracting-Visual-Text)\n",
    "    * [III. Detecting Image Object](#III.-Detecting-Image-Object)\n",
    "* [Natural Language Processing](#Natural-Language-Processing)\n",
    "* [Search Results](#Search-Results)\n",
    "* [Further Research](#Further-Research)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0af2cf",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "Applications for video sharing and storage may be able to enhance user experience by allowing users to search for videos by their content, such as specific words or objects in the video. One of the most popular video sharing apps right now is TikTok where users can save the videos they like to their profile but yet cannot search through the liked videos. \n",
    "\n",
    "As it lacks that functionality, its millions of users are forced to scroll through every single video they have ever liked to find one single clip, and over again. To address this problem, I create a library of TikTok videos and build a search engine that breaks down the videos into several features and returns exact matches to any given query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5a01b",
   "metadata": {},
   "source": [
    "## Data and Methodology\n",
    "A sample of 140 videos are provided in the [videos](https://github.com/czarinagluna/ml-video-library/tree/main/data/videos) folder of this repository for the purpose of demonstrating the end-to-end process I performed. This set of sample is originally saved from my personal user account, and in addition, I downloaded two datasets each containing 1000 videos from Kaggle (found [here](https://www.kaggle.com/datasets/marqueurs404/tiktok-trending-videos) and [here](https://www.kaggle.com/datasets/erikvdven/tiktok-trending-december-2020?select=videos)). Altogether I analyzed over 2000 videos for the whole project, which I uploaded on [Google Drive](https://drive.google.com/drive/folders/1-OMkbBMzBGWH9PVU0ojZACtnFlP3ANbE?usp=sharing). You may download all the videos to explore the complete dataset.\n",
    "\n",
    "**Multimedia Data**\n",
    "\n",
    "A video is a complex data type that can be broken down in a lot of different ways. Through feature engineering, I turned the raw videos into multiple data features that I extracted using the following approaches:\n",
    "- Converting the video to audio and transcribing the speech\n",
    "- Breaking down the video as a sequence of images or frames\n",
    "    - Recognizing on-screen text in the video frames\n",
    "    - Detecting image objects in the video frames\n",
    "\n",
    "**Data Processing**\n",
    "\n",
    "1. Audio processing using `moviepy`, `pydub`, and `speech_recognition`\n",
    "2. Optical character recognition using `opencv-python`, `PIL`, and `pytesseract`\n",
    "3. Object detection using `opencv-python` and `YOLOv3` algorithm\n",
    "\n",
    "Using the above packages and models, the features are extracted as text and so I applied Natural Language Processing (NLP) to process the text and to create a corpus of all the words to search through. Lastly, I built the search engine using `BM25` and deployed the full app via Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd38a9",
   "metadata": {},
   "source": [
    "## Video Processing\n",
    "\n",
    "To start, let's create a dataframe containing the file paths of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287f0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of names of all the video files\n",
    "import os\n",
    "directory = 'data/videos'\n",
    "file_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    f = os.path.join(directory, file)\n",
    "    if os.path.isfile(f):\n",
    "        file_list.append(f)\n",
    "        \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210c4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list into a pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(file_list, columns=['file_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f0c246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/videos/v09044b10000bt9ch34evc02dolrmnm0.mov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/videos/v090440c0000btvqv225mcbk472mqcfg.MP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/videos/v09044650000bqp6ame0bkbl9lnj30ng.MP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/videos/v090440c0000bu92lni5mcbk473oa89g.MP4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/videos/v09044f20000btekl246h3878d1mjtcg.MP4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file_path\n",
       "0  data/videos/v09044b10000bt9ch34evc02dolrmnm0.mov\n",
       "1  data/videos/v090440c0000btvqv225mcbk472mqcfg.MP4\n",
       "2  data/videos/v09044650000bqp6ame0bkbl9lnj30ng.MP4\n",
       "3  data/videos/v090440c0000bu92lni5mcbk473oa89g.MP4\n",
       "4  data/videos/v09044f20000btekl246h3878d1mjtcg.MP4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows of the table\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cfca1",
   "metadata": {},
   "source": [
    "The file paths are used to find the videos. For example, let's display the first video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cf178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_0 = data.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0629cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"data/videos/v09044b10000bt9ch34evc02dolrmnm0.mov\" controls  width=\"300\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Display module from IPython to play a video\n",
    "from IPython.display import Video\n",
    "Video(video_0, width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee4a43",
   "metadata": {},
   "source": [
    "### I. Audio Processing\n",
    "\n",
    "The first feature to extract from the videos is audio. Audio processing is the fastest part in the full process of feature engineering. \n",
    "\n",
    "- To create the audio file, the video formatted as *mov* or *mp4* is converted to a *wav* file. \n",
    "- The audio file is sliced into smaller chunks of audio, split by silence of 500 milliseconds or more, for faster processing. \n",
    "- To recognize the contents, I create an instance of the `Recognizer` class and call the method `recognize_google`. \n",
    "- The full transcription is stored in a text file, which is returned by the function I define as `transcribe_audio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8424c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries and modules\n",
    "import os\n",
    "import speech_recognition as sr \n",
    "from moviepy.editor import AudioFileClip\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    '''\n",
    "    Converts video to audio and returns audio transcription.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): file path of video to be transcribed.\n",
    "    \n",
    "    Returns:\n",
    "    full_text (str): full text transcription of video's audio.\n",
    "    '''\n",
    "    # Write the audio file from the video using MoviePy\n",
    "    # to convert the MP4 or MOV video format to a wav file\n",
    "    transcribed_audio_file = './data/audio/transcribed_audio.wav'\n",
    "    audioclip = AudioFileClip(file_path)\n",
    "    audioclip.write_audiofile(transcribed_audio_file)\n",
    "\n",
    "    try:\n",
    "        sound = AudioSegment.from_file(file_path, 'mp3')\n",
    "    except:\n",
    "        sound = AudioSegment.from_file(file_path, format='mp4')    \n",
    "\n",
    "    # Split the wav file into chunks where there is silence \n",
    "    # for 500 milliseconds or more using PyDub\n",
    "    chunks = split_on_silence(sound, min_silence_len = 500, \n",
    "                              silence_thresh = sound.dBFS-14, keep_silence=500)\n",
    "\n",
    "    # Create a folder called audio_chunks to save the chunks of wav files\n",
    "    folder_name = './data/audio/audio_chunks'\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "        \n",
    "    full_text = ''\n",
    "    \n",
    "    # Create an instance of the Recognizer class from SpeechRecognition\n",
    "    r = sr.Recognizer()\n",
    "    \n",
    "    # Call the method that uses Google Speech Recognition API\n",
    "    # to transcribe the audio and return a string of text\n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        chunk_filename = os.path.join(folder_name, f'chunk{i}.wav')\n",
    "        audio_chunk.export(chunk_filename, format='wav')\n",
    "\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print('Error:', str(e))\n",
    "            else:\n",
    "                text = f'{text.capitalize()}.'\n",
    "                print(chunk_filename, ':', text)\n",
    "                full_text += text\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac312b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function and add a column to the table for the audio transcription\n",
    "data['audio_text'] = data['file_path'].apply(transcribe_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3496e1",
   "metadata": {},
   "source": [
    "### II. Extracting Visual Text\n",
    "\n",
    "The other features to extract from the videos are visual content. Extracting visual text is a faster process than extracting the visual objects.\n",
    "- To create the sequence of images, the video is broken down into frames by `VideoCapture` in the function I define as `save_frames`.\n",
    "- The images are captured every *n*th frame and then opened with the python imaging library.\n",
    "- To recognize the text, the image frames are passed onto the method `image_to_string`.\n",
    "- The extracted text is returned by the function I define as `extract_visual_text` and processed later using NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6aedf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries and modules\n",
    "import cv2\n",
    "import pytesseract\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490b335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_frames = './data/images/image_frames'\n",
    "\n",
    "def save_frames(file_path):\n",
    "    '''\n",
    "    Creates image folder and saves video frames in the folder.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): file path of video to be captured as images.\n",
    "    \n",
    "    Returns:\n",
    "    image_frames folder where the video frames are stored.\n",
    "    '''\n",
    "    try:\n",
    "        os.remove(image_frames)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Create a folder called image_frames to save the images or frames of the video\n",
    "    if not os.path.exists(image_frames):\n",
    "        os.makedirs(image_frames)\n",
    "    \n",
    "    # Capture every 20th frame of the video using cv2 from OpenCV and save to folder\n",
    "    src_vid = cv2.VideoCapture(file_path)\n",
    "\n",
    "    index = 0\n",
    "    while src_vid.isOpened():\n",
    "        ret, frame = src_vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        name = './data/images/image_frames/frame' + str(index) + '.png'\n",
    "\n",
    "        if index % 20 == 0:\n",
    "            print('Extracting frames...' + name)\n",
    "            cv2.imwrite(name, frame)\n",
    "        index = index + 1\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "  \n",
    "    src_vid.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6fd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_alphanumeric(name_list):\n",
    "    '''\n",
    "    Sorts names according to alphanumeric characters.\n",
    "    \n",
    "    Parameters:\n",
    "    name_list (list): list of names to be sorted.\n",
    "    \n",
    "    Returns:\n",
    "    sorted_names (list): sorted list using natural sorting e.g. 1, 2, 10 rather than 1, 10, 2\n",
    "    '''\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)] \n",
    "    sorted_names = sorted(name_list, key=alphanum_key)\n",
    "    return sorted_names\n",
    "\n",
    "# Credits to user136036 for this function found on stack overflow\n",
    "# https://stackoverflow.com/questions/4813061/non-alphanumeric-list-order-from-os-listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe6b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_visual_text(file_path):\n",
    "    '''\n",
    "    Extracts visual text from images saved of video frames.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): file path of video from which to extract the visual text.\n",
    "    \n",
    "    Returns:\n",
    "    full_text (str): text as seen in the video taken from every 20th frame.\n",
    "    '''\n",
    "    save_frames(file_path)\n",
    "    print('Folder created.')\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    # Sort the frames in the folder using the function above for correct ordering\n",
    "    image_list = sorted_alphanumeric(os.listdir(image_frames))\n",
    "    \n",
    "    # Open each image frame using PIL, and pass as argument in a function that uses \n",
    "    # Google Tesseract OCR to recognize text in the image\n",
    "    for i in image_list:\n",
    "        print(str(i))\n",
    "        single_frame = Image.open(image_frames + '/' + i)\n",
    "        text = pytesseract.image_to_string(single_frame, lang='eng')\n",
    "        text_list.append(text)\n",
    "\n",
    "    # Remove the new line character `\\n` and the word TikTok \n",
    "    # from the strings of text returned and joined together\n",
    "    full_text = ' '.join([i for i in text_list])\n",
    "    full_text = full_text.replace('\\n', '').replace('\\x0c', '').replace('TikTok', '')\n",
    "\n",
    "    # Remove the folder to erase the image frames of the video\n",
    "    shutil.rmtree('./data/images/image_frames/')\n",
    "    print('Folder removed.')\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d60158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function and add a column to the table for the visual text\n",
    "data['visual_text'] = data['file_path'].apply(extract_visual_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cac646",
   "metadata": {},
   "source": [
    "#### Extracting Username\n",
    "\n",
    "To extract the username from the visual text, I define the function `extract_username` that executes the following steps:\n",
    "\n",
    "- Create a list of all words in the string of words lowercased.\n",
    "- Create another list of the strings that start with the sign '@'.\n",
    "- Return the most frequent word in the list using the function `most_frequent`.\n",
    "\n",
    "The most frequent word is most likely the username because TikTok automatically displays it for the full duration of the video, as in every single video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de9a6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(username_list):\n",
    "    '''Takes in a list of strings and return the most frequent word in the list or none.'''\n",
    "    most_frequent = max(set(username_list), key = username_list.count)\n",
    "    if most_frequent == '':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return most_frequent\n",
    "\n",
    "def extract_username(visual_text):\n",
    "    '''\n",
    "    Lists possible usernames from visual text and returns the most frequent one that may most likely be the username.\n",
    "    \n",
    "    Parameters:\n",
    "    visual_text (str): full visual text extracted from video.\n",
    "    \n",
    "    Returns:\n",
    "    username (str): most frequent word that starts with @ sign; if none, returns none.\n",
    "    '''\n",
    "    visual_text = ''.join([i for i in visual_text.lower() if not i.isdigit()])\n",
    "    \n",
    "    text = ' '.join(visual_text.split())\n",
    "    text_list = [word for word in text.lower().split()]\n",
    "\n",
    "    username_list = []\n",
    "    for word in text_list:\n",
    "        if re.search(r'[@]', word):\n",
    "            username_list.extend([word.rsplit('@')[-1]])\n",
    "    if username_list == []:\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "        username_list = ' '.join([username for username in username_list])\n",
    "        username_list = [username for username in username_list.strip().split()]\n",
    "        try:\n",
    "            return most_frequent(username_list)\n",
    "        except:\n",
    "            return ' '.join(username_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e91a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function and add a column to the table for the username \n",
    "data['username'] = data['visual_text'].apply(extract_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eea52b",
   "metadata": {},
   "source": [
    "### III. Detecting Image Object\n",
    "\n",
    "The final feature to extract are objects in the videos, which is accomplished by the state-of-the-art object detection system YOLO that uses a deep learning algorithm.\n",
    "\n",
    "**Deep Learning**\n",
    "\n",
    "[YOLO](https://pjreddie.com/darknet/yolo/) applies a convolutional neural network with a network architecture illustrated as such:\n",
    "![](data/images/cnn.png)\n",
    "\n",
    "*Image Source: https://arxiv.org/pdf/1506.02640.pdf*\n",
    "\n",
    "***\n",
    "\n",
    "Compared to prior object detection systems YOLO uses a totally different approach—the network looks at the image once. Thus, the name *You Only Look Once*.\n",
    "- The input image is divided into a grid of *x* by *x* number of cells. \n",
    "- Around the cells, bounding boxes are predicted with confidence scores.\n",
    "- Class probabilities are mapped, with the bounding boxes weighted by the predictions.\n",
    "- The output of objects detected are displayed if the threshold set is met.\n",
    "\n",
    "**Transfer Learning**\n",
    "\n",
    "The weights from the YOLO pre-trained network model are adapted to our data. To load the network model, download the weight and configuration files from Darknet. The configuration file describes the layout of the network by block.\n",
    "\n",
    "> \"YOLOv3 is extremely fast and accurate. In mAP measured at .5 IOU YOLOv3 is on par with Focal Loss but about 4x faster. Moreover, you can easily tradeoff between speed and accuracy simply by changing the size of the model, no retraining required!\" ([Darknet](https://pjreddie.com/darknet/yolo))\n",
    "\n",
    "Choosing `YOLOv3-spp` for accuracy, the model with the highest mean average precision of 60.6 performed on the COCO [dataset](https://cocodataset.org/#home) and for speed, you may try the model with the highest frame per second of 220 which is `YOLOv3-tiny`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44082d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network model into OpenCV using the configuration and weight files \n",
    "net = cv2.dnn.readNetFromDarknet('./data/yolo/yolov3-spp.cfg', './data/yolo/yolov3-spp.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b0503",
   "metadata": {},
   "source": [
    "This YOLO neural network consists of 263 parts such as convolutional layers (`conv`), batch normalization (`bn`) etc. \n",
    "\n",
    "Printing them...\n",
    "\n",
    "`ln = net.getLayerNames()\n",
    "print(len(ln), ln)`\n",
    "\n",
    "...returns the following:\n",
    "\n",
    "`263 ('conv_0', 'bn_0', 'leaky_1', 'conv_1', 'bn_1', 'leaky_2', 'conv_2', 'bn_2', 'leaky_3', 'conv_3', 'bn_3', 'leaky_4', 'shortcut_4', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'conv_7', 'bn_7', 'leaky_8', 'shortcut_8', 'conv_9', 'bn_9', 'leaky_10', ...)`\n",
    "\n",
    "***\n",
    "\n",
    "**COCO**\n",
    "\n",
    "To get the labels of the model trained on the COCO dataset, download the [COCO](https://github.com/pjreddie/darknet/blob/master/data/coco.names) name file that contains the names of all the classes—the model can detect a total of 80 objects. A full list of object classes is provided in the name file, along with the weights and configuration files available in the [data](https://github.com/czarinagluna/ml-video-library/tree/main/data/yolo) folder of the repository.\n",
    "\n",
    "- The objects detected are returned by the function I define as `detect_object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62dcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_object(file_path):\n",
    "    '''\n",
    "    Uses YOLO algorithm to detect objects in video frames.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): file path of video from which to detect objects in the frames.\n",
    "    \n",
    "    Returns:\n",
    "    object_set (list): list of unique objects detected in the video.\n",
    "    '''\n",
    "    classes = []\n",
    "\n",
    "    with open('./data/yolo/coco.names', 'r') as f:\n",
    "        classes = f.read().splitlines()\n",
    "  \n",
    "    try:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        count = 0\n",
    "        object_list = []\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if ret:\n",
    "                cv2.imwrite('frame{:d}.jpg'.format(count), img)\n",
    "                count += 50\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
    "\n",
    "                height, width, _ = img.shape\n",
    "                blob = cv2.dnn.blobFromImage(img, 1/255, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "                net.setInput(blob)\n",
    "\n",
    "                output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "                layerOutputs = net.forward(output_layers_names)\n",
    "\n",
    "                boxes = []\n",
    "                confidences = []\n",
    "                class_ids = []\n",
    "\n",
    "                for output in layerOutputs:\n",
    "                    for detection in output:\n",
    "                        scores = detection[5:]\n",
    "                        class_id = np.argmax(scores)\n",
    "                        confidence = scores[class_id]\n",
    "                        if confidence > 0.5:\n",
    "                            center_x = int(detection[0]*width)\n",
    "                            center_y = int(detection[1]*height)\n",
    "                            w = int(detection[2]*width)\n",
    "                            h = int(detection[3]*height)\n",
    "                            x = int(center_x - w/2)\n",
    "                            y = int(center_y - h/2)\n",
    "                            boxes.append([x, y, w, h])\n",
    "                            confidences.append(float(confidence))\n",
    "                            class_ids.append(class_id)\n",
    "                print(len(boxes))\n",
    "                indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "                if len(indexes) > 0:\n",
    "                    print(indexes.flatten())\n",
    "                    for i in indexes.flatten():\n",
    "                        label = str(classes[class_ids[i]])\n",
    "                    object_list.append(label)\n",
    "            else:\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "            \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        object_set = list(set(object_list))\n",
    "        \n",
    "        print('Done detecting object in this video.')\n",
    "        print(f'These are the objects detected: {object_set}')\n",
    "    \n",
    "        return object_set\n",
    "    \n",
    "    except:\n",
    "        print(f'{filename} did not work.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62505ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function and add a column to the table for the list of objects detected\n",
    "data['object_list'] = data['file_path'].apply(detect_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad9ab4",
   "metadata": {},
   "source": [
    "**Feature Engineering Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce98dac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>visual_text</th>\n",
       "      <th>username</th>\n",
       "      <th>object_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/videos/v09044b10000bt9ch34evc02dolrmnm0.mov</td>\n",
       "      <td>Just came home from work.To find this.On our b...</td>\n",
       "      <td>eelee age Brebs ryJ@ 7 a)D we yamked...</td>\n",
       "      <td>lexwolff</td>\n",
       "      <td>[person, bed, book]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/videos/v090440c0000btvqv225mcbk472mqcfg.MP4</td>\n",
       "      <td></td>\n",
       "      <td>PASTA Di pi pasta without him knowingWheat Td!...</td>\n",
       "      <td>abh.giving</td>\n",
       "      <td>[person, bowl, diningtable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/videos/v09044650000bqp6ame0bkbl9lnj30ng.MP4</td>\n",
       "      <td></td>\n",
       "      <td>n WrteiticentPowder   Powder o Faced - Peachu...</td>\n",
       "      <td>jhk.cof</td>\n",
       "      <td>[person, cell phone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/videos/v090440c0000bu92lni5mcbk473oa89g.MP4</td>\n",
       "      <td></td>\n",
       "      <td>» QUICK/EASYRH“HEALTHY MEAL) = &gt;» QUICK/EASYHE...</td>\n",
       "      <td>gusmevean</td>\n",
       "      <td>[person, pizza, chair, bowl, spoon, cup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/videos/v09044f20000btekl246h3878d1mjtcg.MP4</td>\n",
       "      <td>Play wherever the b**** that ain't got no ass ...</td>\n",
       "      <td>ob @kamiorellano@ How u do that @S®@Dteach us ...</td>\n",
       "      <td>kamiorellano</td>\n",
       "      <td>[person, chair]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>data/videos/v09044080000btsg4svg6g0t4bro25q0.MP4</td>\n",
       "      <td>Honda huntington beach can we end of the s.</td>\n",
       "      <td>most- worn jeans most- worn jeansf d rcmost- w...</td>\n",
       "      <td>vivianeaudiiiob</td>\n",
       "      <td>[person, handbag, refrigerator]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>data/videos/v09044c10000bud34lrgnk9tslqmsi80.MP4</td>\n",
       "      <td>Guy let's play.</td>\n",
       "      <td>isThis NY restaurant will “make you feel like ...</td>\n",
       "      <td>amorraytravels</td>\n",
       "      <td>[person, laptop, chair, bus, umbrella, bottle,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>data/videos/v09044100000brk27uahq10bgjc7ags0.MP4</td>\n",
       "      <td>One day somebody who changes my mind.</td>\n",
       "      <td>ob @jannamoreau       ob @jannamoreau         ...</td>\n",
       "      <td>jannamoreau</td>\n",
       "      <td>[person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>data/videos/v090449d0000br13ppm0bkbnmcmu533g.MP4</td>\n",
       "      <td>Do they record your phone and you're moving to...</td>\n",
       "      <td>ob @angpark oft  4@angpark A op  4 2@angpark i...</td>\n",
       "      <td>angpark</td>\n",
       "      <td>[person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>data/videos/v09044cf0000btags2ipe396hof02f60.MP4</td>\n",
       "      <td>Are you still taking photos like a taurus next...</td>\n",
       "      <td>od @jessidawengofficial ; ay Gc) ®VIDEO PHOT( ...</td>\n",
       "      <td>jessicawangofficial</td>\n",
       "      <td>[person, handbag]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_path  \\\n",
       "0    data/videos/v09044b10000bt9ch34evc02dolrmnm0.mov   \n",
       "1    data/videos/v090440c0000btvqv225mcbk472mqcfg.MP4   \n",
       "2    data/videos/v09044650000bqp6ame0bkbl9lnj30ng.MP4   \n",
       "3    data/videos/v090440c0000bu92lni5mcbk473oa89g.MP4   \n",
       "4    data/videos/v09044f20000btekl246h3878d1mjtcg.MP4   \n",
       "..                                                ...   \n",
       "135  data/videos/v09044080000btsg4svg6g0t4bro25q0.MP4   \n",
       "136  data/videos/v09044c10000bud34lrgnk9tslqmsi80.MP4   \n",
       "137  data/videos/v09044100000brk27uahq10bgjc7ags0.MP4   \n",
       "138  data/videos/v090449d0000br13ppm0bkbnmcmu533g.MP4   \n",
       "139  data/videos/v09044cf0000btags2ipe396hof02f60.MP4   \n",
       "\n",
       "                                            audio_text  \\\n",
       "0    Just came home from work.To find this.On our b...   \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4    Play wherever the b**** that ain't got no ass ...   \n",
       "..                                                 ...   \n",
       "135        Honda huntington beach can we end of the s.   \n",
       "136                                    Guy let's play.   \n",
       "137              One day somebody who changes my mind.   \n",
       "138  Do they record your phone and you're moving to...   \n",
       "139  Are you still taking photos like a taurus next...   \n",
       "\n",
       "                                           visual_text             username  \\\n",
       "0              eelee age Brebs ryJ@ 7 a)D we yamked...             lexwolff   \n",
       "1    PASTA Di pi pasta without him knowingWheat Td!...           abh.giving   \n",
       "2     n WrteiticentPowder   Powder o Faced - Peachu...              jhk.cof   \n",
       "3    » QUICK/EASYRH“HEALTHY MEAL) = >» QUICK/EASYHE...            gusmevean   \n",
       "4    ob @kamiorellano@ How u do that @S®@Dteach us ...         kamiorellano   \n",
       "..                                                 ...                  ...   \n",
       "135  most- worn jeans most- worn jeansf d rcmost- w...      vivianeaudiiiob   \n",
       "136  isThis NY restaurant will “make you feel like ...       amorraytravels   \n",
       "137  ob @jannamoreau       ob @jannamoreau         ...          jannamoreau   \n",
       "138  ob @angpark oft  4@angpark A op  4 2@angpark i...              angpark   \n",
       "139  od @jessidawengofficial ; ay Gc) ®VIDEO PHOT( ...  jessicawangofficial   \n",
       "\n",
       "                                           object_list  \n",
       "0                                  [person, bed, book]  \n",
       "1                          [person, bowl, diningtable]  \n",
       "2                                 [person, cell phone]  \n",
       "3             [person, pizza, chair, bowl, spoon, cup]  \n",
       "4                                      [person, chair]  \n",
       "..                                                 ...  \n",
       "135                    [person, handbag, refrigerator]  \n",
       "136  [person, laptop, chair, bus, umbrella, bottle,...  \n",
       "137                                           [person]  \n",
       "138                                           [person]  \n",
       "139                                  [person, handbag]  \n",
       "\n",
       "[140 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna('')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9af1a",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "To process the text features, I utilize the Natural Language Toolkit (`nltk`) library for standardization to make the letters lowercase, to remove punctuation marks and stopwords, for tokenization and lemmatization. [`WordSegment`](https://pypi.org/project/wordsegment/) is used too, to segment the strings of words that did not have spaces between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00754800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert speech transcribed to lowercase and remove full stop\n",
    "data['standardized_audio_text'] = data['audio_text'].apply(lambda x: x.lower().replace('.', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86c61617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/czarinaluna/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "import wordsegment\n",
    "wordsegment.load()\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "280db0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_visual_text(text):\n",
    "    '''Processes string of text by removing punctuation marks and segment words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'([^A-Za-z0-9|\\s|[:punct:]]*)', '', text)\n",
    "    text = text.replace('|', '').replace(':', '')\n",
    "    text = wordsegment.segment(text) \n",
    "    text = ' '.join([i for i in text if i in words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe8a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function and add a column to the table for the processed visual text\n",
    "data['processed_visual_text'] = data['visual_text'].apply(process_visual_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c9e9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text(text):\n",
    "    '''Segments strings of words without spaces between them.'''\n",
    "    text = wordsegment.segment(text)\n",
    "    text = ' '.join([i for i in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "043183ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to string and add a column to the table for the processed objects\n",
    "data['object_text'] = data['object_list'].apply(lambda x: ' '.join([word for word in x]))\n",
    "\n",
    "data['object_text'] = data['object_text'].apply(segment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af261963",
   "metadata": {},
   "source": [
    "**Text Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "408f856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the features together\n",
    "data['text'] = data['standardized_audio_text'] + ' ' + data['visual_text'] + ' ' + data['processed_visual_text'] + ' ' + data['username'] + ' ' + data['object_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "508e69d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/czarinaluna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/czarinaluna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend([\"i've\", \"let's\", \"lets\", \"youve\", \"theyve\", \"they've\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b31aa5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''Processes text by lemmatization and stopwords removal.'''\n",
    "    text = ' '.join([i for i in text.split() if len(i)>3])\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77ee450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function and add a column to the table for the processed text\n",
    "data['preprocessed_text'] = data['text'].fillna('').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63493069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset\n",
    "data.to_csv('./data/data.csv', index=False)\n",
    "%store data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6735d",
   "metadata": {},
   "source": [
    "## Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64c60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokenized_text = [] \n",
    "\n",
    "for doc in tqdm(nlp.pipe(data['preprocessed_text'].fillna('').str.lower().values, disable=['tagger', 'parser', 'ner'])):\n",
    "    tokenized = [token.text for token in doc if token.is_alpha]\n",
    "    tokenized_text.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b75de046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf7b906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_video(query, result=3, n=1):\n",
    "    '''\n",
    "    Uses YOLO algorithm to detect objects in video frames.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): word or phrases to search.\n",
    "    result (int): the number of results to search.\n",
    "    n (int): the nth result to display.\n",
    "    \n",
    "    Returns:\n",
    "    video to display from the list of results.\n",
    "    '''\n",
    "    tokenized_query = query.lower().split(' ')\n",
    "    \n",
    "    results = bm25.get_top_n(tokenized_query, data['file_path'], result)\n",
    "    results_list = [video for video in results]\n",
    "\n",
    "    video = Video(results_list[n-1], width=300)\n",
    "    print(results_list[n-1])\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0e5a848",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/videos/v090440e0000bttubbe6r5jsiqc8bji0.MP4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"data/videos/v090440e0000bttubbe6r5jsiqc8bji0.MP4\" controls  width=\"300\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_video('chinatown dumplings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84286c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/videos/v09044c10000bud34lrgnk9tslqmsi80.MP4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"data/videos/v09044c10000bud34lrgnk9tslqmsi80.MP4\" controls  width=\"300\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_video('italian restaurant ny umbrella')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8659571",
   "metadata": {},
   "source": [
    "## Further Research\n",
    "\n",
    "Further developments that could add value to the model are **music recognition** to resolve the limits of speech recognition, and application of a **neural network-based search** feature to further improve accuracy. Developing this on **other applications** for video sharing and storage such as Apple Photos could help users better manage videos on their devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3aed35",
   "metadata": {},
   "source": [
    "# Contact\n",
    "Feel free to contact me for any questions and connect with me on [Linkedin](https://www.linkedin.com/in/czarinagluna)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
